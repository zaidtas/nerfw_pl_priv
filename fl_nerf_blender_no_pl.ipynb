{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Add noise\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centralised training: the old way of doing ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating a simple (but complete) training loop as it is commonly done in centralised setups. Starting our tutorial in this way will allow us to very clearly identify which parts of a typical ML pipeline are common to both centralised and federated training.\n",
    "\n",
    "For this tutorial we'll design a image classification pipeline for [MNIST digits](https://en.wikipedia.org/wiki/MNIST_database) and using a simple CNN model as the network to train. The MNIST dataset is comprised of `28x28` greyscale images with digits from 0 to 9 (i.e. 10 classes in total)\n",
    "\n",
    "\n",
    "## A dataset\n",
    "\n",
    "Let's begin by constructing the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '5,7'\n",
    "# we naturally first need to import torch and torchvision\n",
    "\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "from opt import get_opts\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from datasets import dataset_dict\n",
    "\n",
    "# models\n",
    "from models.nerf import *\n",
    "from models.rendering import *\n",
    "\n",
    "# optimizer, scheduler, visualization\n",
    "from utils import *\n",
    "\n",
    "# losses\n",
    "from losses import loss_dict\n",
    "\n",
    "# metrics\n",
    "from metrics import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code is to convert filenames which are step size\n",
    "# import os\n",
    "\n",
    "# # Set the directory path\n",
    "# directory = \"/home/zt16/code/priv-nerf/nerfw_pl_priv/data/lego/res400_360view\"\n",
    "\n",
    "# # Set the starting number for renaming\n",
    "# counter = 6\n",
    "\n",
    "# # Iterate through each file in the directory\n",
    "# for filename in os.listdir(directory):\n",
    "#     # Check if the file is a regular file (not a directory)\n",
    "#     if os.path.isfile(os.path.join(directory, filename)) and counter<100:\n",
    "#         # Get the file extension (if any)\n",
    "#         print(filename)\n",
    "#         ext = os.path.splitext(filename)[1]\n",
    "#         # Generate the new filename\n",
    "#         new_filename = f\"r_{counter:d}{ext}\"\n",
    "#         print(new_filename)\n",
    "#         # Rename the file\n",
    "#         os.rename(os.path.join(directory, filename), os.path.join(directory, new_filename))\n",
    "#         # Increment the counter\n",
    "#         counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataloader(hparams):\n",
    "    dataset = dataset_dict[hparams.dataset_name]\n",
    "    kwargs = {'root_dir': hparams.root_dir}\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if hparams.dataset_name == 'phototourism':\n",
    "        kwargs['img_downscale'] = hparams.img_downscale\n",
    "        kwargs['val_num'] = hparams.num_gpus\n",
    "        kwargs['use_cache'] = hparams.use_cache\n",
    "        kwargs['use_mask'] = hparams.use_mask\n",
    "    elif hparams.dataset_name == 'blender':\n",
    "        kwargs['img_wh'] = tuple(hparams.img_wh)\n",
    "        kwargs['perturbation'] = hparams.data_perturb\n",
    "        kwargs['random_occ'] = not hparams.nonrandom_occ\n",
    "        kwargs['occ_yaw'] = hparams.occ_yaw\n",
    "        kwargs['yaw_threshold'] = hparams.yaw_threshold\n",
    "        kwargs['all_img_occ'] = hparams.all_img_occ\n",
    "    \n",
    "    train_dataset = dataset(split='train', **kwargs)\n",
    "#     full_dataset = dataset(split='train', **kwargs)\n",
    "\n",
    "    \n",
    "#     train_datasets = random_split(train_dataset, lengths, torch.Generator().manual_seed(42))\n",
    "    img_sample_size = hparams.img_wh[0] * hparams.img_wh[1]\n",
    "#     import pdb; pdb.set_trace()\n",
    "    if hparams.public_dataset:\n",
    "        kwargs_public = copy.deepcopy(kwargs)\n",
    "#         kwargs_public['random_occ'] = True \n",
    "        kwargs_public['root_dir'] = hparams.public_root_dir\n",
    "        public_dataset = dataset(split='train',**kwargs_public)\n",
    "        public_train_ray_idx = []\n",
    "        for ind in range(0,100,5):\n",
    "            public_train_ray_idx.extend(list(range(ind*img_sample_size,(ind+1)*img_sample_size)))    \n",
    "        public_train_dataset = Subset(public_dataset,public_train_ray_idx)\n",
    "        public_train_loaders = DataLoader(public_train_dataset,shuffle=True,\n",
    "                              num_workers=4,\n",
    "                              batch_size=hparams.batch_size,\n",
    "                              pin_memory=True)\n",
    "#         import pdb; pdb.set_trace()\n",
    "#         full_idx = set(list(range(img_sample_size*100)))\n",
    "#         public_idx = set(public_train_ray_idx)\n",
    "#         remaining_idx = list(full_idx-public_idx)\n",
    "#         remaining_idx = [x for x in full_idx if x not in public_train_ray_idx]\n",
    "        train_dataset_remaining = train_dataset\n",
    "#         train_dataset_remaining = Subset(train_dataset,remaining_idx)\n",
    "    else:\n",
    "        train_dataset_remaining = train_dataset\n",
    "        public_train_loaders = None\n",
    "    \n",
    "    #splitting the dataset\n",
    "    partition_size = len(train_dataset_remaining) // hparams.num_clients\n",
    "    lengths = [partition_size] * (hparams.num_clients)\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "    train_datasets = []\n",
    "    for ind in range(hparams.num_clients):\n",
    "        train_datasets.append(Subset(train_dataset_remaining,range(ind*partition_size,ind*partition_size+partition_size)))\n",
    "    val_dataset = dataset(split='val', **kwargs)\n",
    "    \n",
    "    train_loaders = []\n",
    "    val_loaders = []\n",
    "    \n",
    "    for trainset in train_datasets:\n",
    "        train_loaders.append(DataLoader(trainset,shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          batch_size=hparams.batch_size,\n",
    "                          pin_memory=True))\n",
    "        val_loaders.append(DataLoader(val_dataset,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4,\n",
    "                          batch_size=1, # validate one image (H*W rays) at a time\n",
    "                          pin_memory=True))\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    return train_loaders, val_loaders, train_dataset, public_train_loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the experiment\n",
    "\n",
    "This tutorial is not so much about novel architectural designs so we keep things simple and make use of a typical CNN that is adequate for the MNIST image classification task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hparams,full_dataset) -> None:\n",
    "        super(Net,self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.full_dataset = full_dataset\n",
    "#         self.loss = loss_dict['nerfw'](coef=1)\n",
    "\n",
    "        self.models_to_train = []\n",
    "        self.embedding_xyz = PosEmbedding(hparams.N_emb_xyz-1, hparams.N_emb_xyz)\n",
    "        self.embedding_dir = PosEmbedding(hparams.N_emb_dir-1, hparams.N_emb_dir)\n",
    "        self.embeddings = {'xyz': self.embedding_xyz,\n",
    "                           'dir': self.embedding_dir}\n",
    "\n",
    "        if hparams.encode_a:\n",
    "            self.embedding_a = torch.nn.Embedding(hparams.N_vocab, hparams.N_a)\n",
    "            self.embeddings['a'] = self.embedding_a\n",
    "            self.models_to_train += [self.embedding_a]\n",
    "        if hparams.encode_t:\n",
    "            self.embedding_t = torch.nn.Embedding(hparams.N_vocab, hparams.N_tau)\n",
    "            self.embeddings['t'] = self.embedding_t\n",
    "            self.models_to_train += [self.embedding_t]\n",
    "\n",
    "        self.nerf_coarse = NeRF('coarse',\n",
    "                                in_channels_xyz=6*hparams.N_emb_xyz+3,\n",
    "                                in_channels_dir=6*hparams.N_emb_dir+3)\n",
    "        self.models = {'coarse': self.nerf_coarse}\n",
    "        if hparams.N_importance > 0:\n",
    "            self.nerf_fine = NeRF('fine',\n",
    "                                  in_channels_xyz=6*hparams.N_emb_xyz+3,\n",
    "                                  in_channels_dir=6*hparams.N_emb_dir+3,\n",
    "                                  encode_appearance=hparams.encode_a,\n",
    "                                  in_channels_a=hparams.N_a,\n",
    "                                  encode_transient=hparams.encode_t,\n",
    "                                  in_channels_t=hparams.N_tau,\n",
    "                                  beta_min=hparams.beta_min)\n",
    "            self.models['fine'] = self.nerf_fine\n",
    "        self.models_to_train += [self.models]\n",
    "\n",
    "    def forward(self, rays, ts):\n",
    "        B = rays.shape[0]\n",
    "        results = defaultdict(list)\n",
    "        for i in range(0, B, self.hparams.chunk):\n",
    "            rendered_ray_chunks = \\\n",
    "                render_rays(self.models,\n",
    "                            self.embeddings,\n",
    "                            rays[i:i+self.hparams.chunk],\n",
    "                            ts[i:i+self.hparams.chunk],\n",
    "                            self.hparams.N_samples,\n",
    "                            self.hparams.use_disp,\n",
    "                            self.hparams.perturb,\n",
    "                            self.hparams.noise_std,\n",
    "                            self.hparams.N_importance,\n",
    "                            self.hparams.chunk, # chunk size is effective in val mode\n",
    "                            self.full_dataset.white_back)\n",
    "\n",
    "            for k, v in rendered_ray_chunks.items():\n",
    "                results[k] += [v]\n",
    "\n",
    "        for k, v in results.items():\n",
    "            results[k] = torch.cat(v, 0)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be training the model in a Federated setting. In order to do that, we need to define two functions:\n",
    "\n",
    "* `train()` that will train the model given a dataloader.\n",
    "* `test()` that will be used to evaluate the performance of the model on held-out data, e.g., a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,trainloader,optimizer,scheduler,epochs,device):    \n",
    "    criterion = loss_dict['nerfw'](coef=1)\n",
    "    net.train()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        itera=0\n",
    "        for batch in trainloader:\n",
    "\n",
    "#             if itera>10:\n",
    "#                 break\n",
    "                \n",
    "            rays, rgbs, ts = batch['rays'], batch['rgbs'], batch['ts']\n",
    "            rays = rays.to(device)\n",
    "            rgbs = rgbs.to(device)\n",
    "            ts = ts.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            results = net(rays,ts)\n",
    "            loss_d = criterion(results,rgbs)\n",
    "            loss = sum(l for l in loss_d.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            itera+=1\n",
    "            \n",
    "                \n",
    "        scheduler.step()\n",
    "#         print('iteration done')\n",
    "    return net\n",
    "\n",
    "def test(net, valloader, device):\n",
    "    criterion = loss_dict['nerfw'](coef=1)\n",
    "    psnr_, loss = 0.0, 0.0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valloader:\n",
    "#             batch.to(device)\n",
    "            rays, rgbs, ts = batch['rays'], batch['rgbs'], batch['ts']\n",
    "            rays = rays.to(device)\n",
    "            rgbs = rgbs.to(device)\n",
    "            ts = ts.to(device)\n",
    "            rays = rays.squeeze() # (H*W, 3)\n",
    "            rgbs = rgbs.squeeze() # (H*W, 3)\n",
    "            ts = ts.squeeze() # (H*W)\n",
    "            results = net(rays, ts)\n",
    "            loss_d = criterion(results, rgbs)\n",
    "            loss += sum(l for l in loss_d.values())\n",
    "            typ = 'fine' if 'rgb_fine' in results else 'coarse'\n",
    "\n",
    "            psnr_ += psnr(results[f'rgb_{typ}'], rgbs)\n",
    "    val_psnr = psnr_ / len(valloader.dataset)\n",
    "            \n",
    "    return loss, val_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--root_dir', type=str, required=False,\n",
    "                    help='root directory of dataset')\n",
    "parser.add_argument('--dataset_name', type=str, default='blender',\n",
    "                    choices=['blender', 'phototourism'],\n",
    "                    help='which dataset to train/val')\n",
    "# for blender\n",
    "parser.add_argument('--data_perturb', nargs=\"+\", type=str, default=[],\n",
    "                    help='''what perturbation to add to data.\n",
    "                            Available choices: [], [\"color\"], [\"occ\"] or [\"color\", \"occ\"]\n",
    "                         ''')\n",
    "parser.add_argument('--nonrandom_occ', default=False, action=\"store_true\",\n",
    "                    help='whether to use non-random occlusion')\n",
    "parser.add_argument('--all_img_occ', default=False, action=\"store_true\",\n",
    "                    help='whether to add black occlusion to all images')\n",
    "parser.add_argument('--occ_yaw', type=float, default=0.0,\n",
    "                    help='yaw angle for selecting images for non-random occlusion')\n",
    "parser.add_argument('--yaw_threshold', type=float, default=0.0,\n",
    "                    help='threshold for selecting images for non-random occlusion')\n",
    "parser.add_argument('--img_wh', nargs=\"+\", type=int, default=[800, 800],\n",
    "                    help='resolution (img_w, img_h) of the image')\n",
    "\n",
    "# for phototourism\n",
    "parser.add_argument('--img_downscale', type=int, default=1,\n",
    "                    help='how much to downscale the images for phototourism dataset')\n",
    "parser.add_argument('--use_cache', default=False, action=\"store_true\",\n",
    "                    help='whether to use ray cache (make sure img_downscale is the same)')\n",
    "parser.add_argument('--use_mask', default=False, action=\"store_true\",\n",
    "                    help='use masked images')\n",
    "\n",
    "# original NeRF parameters\n",
    "parser.add_argument('--N_emb_xyz', type=int, default=10,\n",
    "                    help='number of xyz embedding frequencies')\n",
    "parser.add_argument('--N_emb_dir', type=int, default=4,\n",
    "                    help='number of direction embedding frequencies')\n",
    "parser.add_argument('--N_samples', type=int, default=64,\n",
    "                    help='number of coarse samples')\n",
    "parser.add_argument('--N_importance', type=int, default=128,\n",
    "                    help='number of additional fine samples')\n",
    "parser.add_argument('--use_disp', default=False, action=\"store_true\",\n",
    "                    help='use disparity depth sampling')\n",
    "parser.add_argument('--perturb', type=float, default=1.0,\n",
    "                    help='factor to perturb depth sampling points')\n",
    "parser.add_argument('--noise_std', type=float, default=1.0,\n",
    "                    help='std dev of noise added to regularize sigma')\n",
    "\n",
    "# NeRF-W parameters\n",
    "parser.add_argument('--N_vocab', type=int, default=100,\n",
    "                    help='''number of vocabulary (number of images) \n",
    "                            in the dataset for nn.Embedding''')\n",
    "parser.add_argument('--encode_a', default=False, action=\"store_true\",\n",
    "                    help='whether to encode appearance (NeRF-A)')\n",
    "parser.add_argument('--N_a', type=int, default=48,\n",
    "                    help='number of embeddings for appearance')\n",
    "parser.add_argument('--encode_t', default=False, action=\"store_true\",\n",
    "                    help='whether to encode transient object (NeRF-U)')\n",
    "parser.add_argument('--N_tau', type=int, default=16,\n",
    "                    help='number of embeddings for transient objects')\n",
    "parser.add_argument('--beta_min', type=float, default=0.1,\n",
    "                    help='minimum color variance for each ray')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=1024,\n",
    "                    help='batch size')\n",
    "parser.add_argument('--chunk', type=int, default=32*1024,\n",
    "                    help='chunk size to split the input to avoid OOM')\n",
    "parser.add_argument('--num_epochs', type=int, default=16,\n",
    "                    help='number of training epochs')\n",
    "parser.add_argument('--num_gpus', type=int, default=1,\n",
    "                    help='number of gpus')\n",
    "\n",
    "parser.add_argument('--ckpt_path', type=str, default=None,\n",
    "                    help='pretrained checkpoint path to load')\n",
    "parser.add_argument('--prefixes_to_ignore', nargs='+', type=str, default=['loss'],\n",
    "                    help='the prefixes to ignore in the checkpoint state dict')\n",
    "\n",
    "parser.add_argument('--optimizer', type=str, default='adam',\n",
    "                    help='optimizer type',\n",
    "                    choices=['sgd', 'adam', 'radam', 'ranger'])\n",
    "parser.add_argument('--lr', type=float, default=5e-4,\n",
    "                    help='learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9,\n",
    "                    help='learning rate momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--lr_scheduler', type=str, default='steplr',\n",
    "                    help='scheduler type',\n",
    "                    choices=['steplr', 'cosine', 'poly'])\n",
    "#### params for warmup, only applied when optimizer == 'sgd' or 'adam'\n",
    "parser.add_argument('--warmup_multiplier', type=float, default=1.0,\n",
    "                    help='lr is multiplied by this factor after --warmup_epochs')\n",
    "parser.add_argument('--warmup_epochs', type=int, default=0,\n",
    "                    help='Gradually warm-up(increasing) learning rate in optimizer')\n",
    "###########################\n",
    "#### params for steplr ####\n",
    "parser.add_argument('--decay_step', nargs='+', type=int, default=[20],\n",
    "                    help='scheduler decay step')\n",
    "parser.add_argument('--decay_gamma', type=float, default=0.1,\n",
    "                    help='learning rate decay amount')\n",
    "###########################\n",
    "#### params for poly ####\n",
    "parser.add_argument('--poly_exp', type=float, default=0.9,\n",
    "                    help='exponent for polynomial learning rate decay')\n",
    "###########################\n",
    "\n",
    "parser.add_argument('--exp_name', type=str, default='exp',\n",
    "                    help='experiment name')\n",
    "parser.add_argument('--refresh_every', type=int, default=1,\n",
    "                    help='print the progress bar every X steps')\n",
    "# Federated Learning parameters\n",
    "parser.add_argument('--num_clients', type=int, default=1,help='number of clients')\n",
    "parser.add_argument('--num_rounds', type=int, default=10,help='number of rounds')\n",
    "parser.add_argument('--public_dataset', default=False, action=\"store_true\",\n",
    "                    help='whether to use public dataset for training')\n",
    "hparams = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blender'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code we have written so far is not specific to Federated Learning. Then, what are the key differences between Federated Learning and Centralised Training? If you could only pick you, probably you'd say:\n",
    "* Federated Learning is distributed -- the model is trained on-device by the participating clients.\n",
    "* Data remains private and is owned by a specific _client_ -- the data is never sent to the central server.\n",
    "\n",
    "The are several more differences. But the above two are the main ones to always consider and that are common to all flavours of Federated Learning (e.g. _cross-device_ or _cross-silo_). The remaining of this tutorial is going to focus in transforming the code we have written so far for the centralised setting and construct a Federated Learning pipeline using Flower and PyTorch.\n",
    "\n",
    "Let's begin! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "outputId": "0f53ca81-cb55-46ef-c8e0-4e19a4f060b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add [] perturbation!\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "hparams.dataset_name = 'blender'\n",
    "hparams.root_dir = '/home/zt16/code/priv-nerf/nerfw_pl_priv/data/lego/res800_360view_IID_vertical_random'#'/home/zt16/code/priv-nerf/nerfw_pl_priv/data/lego/res400_360view'\n",
    "hparams.public_root_dir = '/home/zt16/code/priv-nerf/nerfw_pl_priv/data/lego/res400_360view_random'\n",
    "hparams.N_importance = 64\n",
    "hparams.img_wh = [400, 400]\n",
    "hparams.noise_std = 0\n",
    "hparams.num_epochs = 5\n",
    "hparams.batch_size = 1024 \n",
    "hparams.optimizer = 'adam'\n",
    "hparams.lr = 5e-4\n",
    "hparams.lr_scheduler = 'cosine'\n",
    "hparams.exp_name = 'lego_nerfU_nonIID_split_20clients_newdataset'\n",
    "hparams.encode_t = True #False #True\n",
    "hparams.beta_min = 0.1 \n",
    "hparams.data_perturb = [] #[\"occ\"]\n",
    "hparams.num_clients = 20\n",
    "hparams.num_rounds = 30\n",
    "hparams.nonrandom_occ = True\n",
    "hparams.all_img_occ = True\n",
    "hparams.public_dataset = False\n",
    "NUM_CLIENTS = hparams.num_clients\n",
    "    \n",
    "\n",
    "\n",
    "trainloaders,valloaders, full_dataset_central, public_train_loader = setup_dataloader(hparams)\n",
    "hparams_public = copy.deepcopy(hparams)\n",
    "hparams_central = copy.deepcopy(hparams)\n",
    "hparams_public.encode_t = True\n",
    "hparams_central.encode_t = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model_central = Net(hparams,full_dataset_central).to(device)\n",
    "# # # Define the optimizer and schedulter\n",
    "# optim = get_optimizer(hparams, model_central.models_to_train)\n",
    "# scheduler = get_scheduler(hparams, optim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # do local training\n",
    "# train(model_central, public_train_loader, optim, scheduler, epochs=20, device=device)\n",
    "# loss, accuracy = test(model_central, valloaders[0], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loss,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "# _state_dict_all = model_central.state_dict()\n",
    "# _mydict = model_central.state_dict().keys()\n",
    "\n",
    "# _mystr_arr = ['nerf_fine.transient_','embedding_t']\n",
    "# _transient_keys = []\n",
    "# #         import pdb; pdb.set_trace()\n",
    "# for mystr in _mystr_arr:\n",
    "# #             avoid_keys.append(key.startswith(mystr) for key in mydict)\n",
    "#     for key in _mydict:\n",
    "#         if key.startswith(mystr):\n",
    "#             _transient_keys.append(key)\n",
    "\n",
    "\n",
    "\n",
    "# _state_dict_transient = OrderedDict({k: v for k, v in _state_dict_all.items() if k in _transient_keys })\n",
    "# _state_dict_static = OrderedDict({k: v for k, v in _state_dict_all.items() if k not in _transient_keys })\n",
    "\n",
    "# os.system(f'mkdir -p ckpts/{hparams.exp_name}')\n",
    "# _ckpt_file = os.path.join(f'ckpts/{hparams.exp_name}/central_model_transient40.pth')\n",
    "# _ckpt_file_static = os.path.join(f'ckpts/{hparams.exp_name}/central_model_static40.pth')\n",
    "# torch.save(_state_dict_transient,_ckpt_file)\n",
    "# torch.save(_state_dict_static,_ckpt_file_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# public_ckpt_file_static = os.path.join(f'ckpts/{hparams.exp_name}/central_model_static40.pth')\n",
    "# public_static_state_dict = torch.load(public_ckpt_file_static)\n",
    "\n",
    "# public_model_static_params = [val.cpu().numpy() for _,val in public_static_state_dict.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict2=model_central2.state_dict()\n",
    "# # for k, v in state_dict2.items():\n",
    "# #     jk=1\n",
    "# state_dict = OrderedDict({k: v for k, v in state_dict2.items() if k in ['ghgjhj'] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "782"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloaders[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the histogram of this partition is a bit different from the one we obtained at the beginning where we took the entire dataset into consideration. Because our data partitions are artificially constructed by sampling the MNIST dataset in an IID fashion, our Federated Learning example will not face sever _data heterogeneity_ issues (which is a fairly [active research topic](https://arxiv.org/abs/1912.04977)).\n",
    "\n",
    "Let's next define how our FL clients will behave\n",
    "\n",
    "## Defining a Flower Client\n",
    "\n",
    "You can think of a client in FL as an entity that owns some data and trains a model using this data. The caveat is that the model is being trained _collaboratively_ in Federation by multiple clients (sometimes up to hundreds of thousands) and, in most instances of FL, is sent by a central server.\n",
    "\n",
    "A Flower Client is a simple Python class with four distinct methods:\n",
    "\n",
    "* `fit()`: With this method, the client does on-device training for a number of epochs using its own data. At the end, the resulting model is sent back to the server for aggregation.\n",
    "\n",
    "* `evaluate()`: With this method, the server can evaluate the performance of the global model on the local validation set of a client. This can be used for instance when there is no centralised dataset on the server for validation/test. Also, this method can be use to asses the degree of personalisation of the model being federated.\n",
    "\n",
    "* `set_parameters()`: This method takes the parameters sent by the server and uses them to initialise the parameters of the local model that is ML framework specific (e.g. TF, Pytorch, etc).\n",
    "\n",
    "* `get_parameters()`: It extract the parameters from the local model and transforms them into a list of NumPy arrays. This ML framework-agnostic representation of the model will be sent to the server.\n",
    "\n",
    "Let's start by importing Flower!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 11:03:46,068\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.6.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import flwr as fl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's defice our Flower Client class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from flwr.common import NDArrays, Scalar\n",
    "import glob\n",
    "\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def __init__(self, hparams, trainloader, vallodaer,full_dataset,client_number) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.trainloader = trainloader\n",
    "        self.valloader = vallodaer\n",
    "        self.full_dataset = full_dataset\n",
    "        self.hparams = hparams\n",
    "        self.model = Net(self.hparams,self.full_dataset)\n",
    "        self.client_num = client_number\n",
    "        # Determine device\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # figure out Transient params\n",
    "        mydict = self.model.state_dict().keys()\n",
    "#         avoid_transient = ['transient_rgb','transient_encoding','transient_sigma','transient_beta','embedding_t']\n",
    "        mystr_arr = ['nerf_fine.transient_','embedding_t']\n",
    "        transient_keys = []\n",
    "#         import pdb; pdb.set_trace()\n",
    "        for mystr in mystr_arr:\n",
    "#             avoid_keys.append(key.startswith(mystr) for key in mydict)\n",
    "            for key in mydict:\n",
    "                if key.startswith(mystr):\n",
    "                    transient_keys.append(key)\n",
    "        self.transient_keys = transient_keys\n",
    "                    \n",
    "        self.model.to(self.device)  # send model to device\n",
    "        self.epoch_num =1\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        \"\"\"With the model paramters received from the server,\n",
    "        overwrite the uninitialise model in this class with them.\"\"\"\n",
    "#         import pdb; pdb.set_trace()\n",
    "        \n",
    "#         print(avoid_keys)\n",
    "        mykeys = self.model.state_dict().keys()\n",
    "#         import pdb;pdb.set_trace()\n",
    "        mykeys = [key for key in mykeys if key != 'embedding_t.weight'] #embedding_t is non learnable\n",
    "        params_dict = zip(mykeys, parameters)\n",
    "        state_dict_static = OrderedDict({k: torch.Tensor(v) for k, v in params_dict if k not in self.transient_keys })\n",
    "        ckpt_file_dir = os.path.join(f'ckpts/{self.hparams.exp_name}/clients/client_{self.client_num:0>2d}/**')\n",
    "        files = glob.glob(ckpt_file_dir)\n",
    "        if len(files)>0:\n",
    "            \n",
    "            ckpt_file = max(files)\n",
    "#             print(\"loading from saved ckpt: \",ckpt_file)\n",
    "            state_dict_transient = torch.load(ckpt_file)\n",
    "            state_dict_transient =  OrderedDict({k: v for k, v in state_dict_transient.items() if k in self.transient_keys })\n",
    "\n",
    "            state_dict_static.update(state_dict_transient)\n",
    "        \n",
    "        self.model.load_state_dict(state_dict_static, strict=False)\n",
    "\n",
    "    def get_parameters(self, config: Dict[str, Scalar]):\n",
    "        \"\"\"Extract all model parameters and conver them to a list of\n",
    "        NumPy arryas. The server doesn't work with PyTorch/TF/etc.\"\"\"\n",
    "        return [val.cpu().numpy() for key, val in self.model.state_dict().items() if key not in self.transient_keys]\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"This method train the model using the parameters sent by the\n",
    "        server on the dataset of this client. At then end, the parameters\n",
    "        of the locally trained model are communicated back to the server\"\"\"\n",
    "        \n",
    "        # read from config\n",
    "        server_round_, lr, epochs = config[\"round\"], config[\"lr\"], config[\"epochs\"]\n",
    "        \n",
    "        # copy parameters sent by the server into client's local model\n",
    "        self.set_parameters(parameters)\n",
    "\n",
    "        # Define the optimizer and schedulter\n",
    "        optim = get_optimizer(self.hparams, self.model.models_to_train)\n",
    "        scheduler = get_scheduler(self.hparams, optim)\n",
    "\n",
    "        # do local training\n",
    "        train(self.model, self.trainloader, optim, scheduler, epochs=epochs, device=self.device)\n",
    "                \n",
    "        state_dict_all = self.model.state_dict()\n",
    "        \n",
    "        if hparams.encode_t:\n",
    "            state_dict_transient = OrderedDict({k: v for k, v in state_dict_all.items() if k in self.transient_keys })\n",
    "            ckpt_file = os.path.join(f'ckpts/{self.hparams.exp_name}/clients/client_{self.client_num:0>2d}/epoch_{server_round_:0>2d}.pth')\n",
    "            torch.save(state_dict_transient,ckpt_file)\n",
    "        \n",
    "        state_dict_static = OrderedDict({k: v for k, v in state_dict_all.items() if k not in self.transient_keys })        \n",
    "        ckpt_file_static = os.path.join(f'ckpts/{self.hparams.exp_name}/clients-static/client_{self.client_num:0>2d}/epoch_{server_round_:0>2d}.pth')        \n",
    "        torch.save(state_dict_static,ckpt_file_static)\n",
    "\n",
    "        # return the model parameters to the server as well as extra info (number of training examples in this case)\n",
    "        return self.get_parameters({}), len(self.trainloader), {}\n",
    "\n",
    "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]):\n",
    "        \"\"\"Evaluate the model sent by the server on this client's\n",
    "        local validation set. Then return performance metrics.\"\"\"\n",
    "        self.set_parameters(parameters)\n",
    "        loss, accuracy = test(self.model, self.valloader, device=self.device)\n",
    "        # send statistics back to the server\n",
    "        return float(loss), len(self.valloader), {\"val_psnr\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams.encode_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client1 = FlowerClient(hparams, trainloaders[0], valloaders[0],full_dataset_central,client_number=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend a few minutes to inspect the `FlowerClient` class above. Please ask questions if there is something unclear !\n",
    "\n",
    "Then keen-eyed among you might have realised that if we were to fuse the client's `fit()` and `evaluate()` methods, we'll end up with essentially the same as in the `run_centralised()` function we used in the Centralised Training part of this tutorial. And it is true!! In Federated Learning, the way clients perform local training makes use of the same principles as more traditional centralised setup. The key difference is that the dataset now is much smaller and it's never _\"seen\"_ by the entity running the FL workload (i.e. the central server).\n",
    "\n",
    "\n",
    "Talking about the central server... we should define what strategy we want to make use of so the updated models sent from the clients back to the server at the end of the `fit()` method are aggregate.\n",
    "\n",
    "\n",
    "## Choosing a Flower Strategy\n",
    "\n",
    "\n",
    "A strategy sits at the core of the Federated Learning experiment. It is involved in all stages of a FL pipeline: sampling clients; sending the _global model_ to the clients so they can do `fit()`; receive the updated models from the clients and **aggregate** these to construct a new _global model_; define and execute global or federated evaluation; and more.\n",
    "\n",
    "Flower comes with [many strategies built-in](https://github.com/adap/flower/tree/main/src/py/flwr/server/strategy) and more to be available in the next release (`1.5` already!). For this tutorial, let's use what is arguable the most popular strategy out there: `FedAvg`.\n",
    "\n",
    "The way `FedAvg` works is simple but performs surprisingly well in practice. It is therefore one good strategy to start your experimentation. `FedAvg`, as its name implies, derives a new version of the _global model_ by taking the average of all the models sent by clients participating in the round. You can read all the details [in the paper](https://arxiv.org/abs/1602.05629).\n",
    "\n",
    "Let's see how we can define `FedAvg` using Flower. We use one of the callbacks called `evaluate_fn` so we can easily evaluate the state of the global model using a small centralised testset. Note this functionality is user-defined since it requires a choice in terms of ML-framework. (if you recall, Flower is framework agnostic).\n",
    "\n",
    "> This being said, centralised evaluation of the global model is only possible if there exists a centralised dataset that somewhat follows a similar distribution as the data that's spread across clients. In some cases having such centralised dataset for validation is not possible, so the only solution is to federate the evaluation of the _global model_. This is the default behaviour in Flower. If you don't specify teh `evaluate_fn` argument in your strategy, then, centralised global evaluation won't be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluate_fn(testloader):\n",
    "    \"\"\"This is a function that returns a function. The returned\n",
    "    function (i.e. `evaluate_fn`) will be executed by the strategy\n",
    "    at the end of each round to evaluate the stat of the global\n",
    "    model.\"\"\"\n",
    "\n",
    "    def evaluate_fn(server_round: int, parameters, config):\n",
    "        \"\"\"This function is executed by the strategy it will instantiate\n",
    "        a model and replace its parameters with those from the global model.\n",
    "        The, the model will be evaluate on the test set (recall this is the\n",
    "        whole MNIST test set).\"\"\"\n",
    "\n",
    "        central_model = Net(hparams_central,full_dataset_central)\n",
    "\n",
    "        # Determine device\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        central_model.to(device)  # send model to device\n",
    "\n",
    "#         print(\"global model evaluation started\")\n",
    "        # set parameters to the model\n",
    "#         print(central_model.state_dict().keys())\n",
    "        params_dict = zip(central_model.state_dict().keys(), parameters)\n",
    "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
    "        central_model.load_state_dict(state_dict, strict=True)\n",
    "        \n",
    "#         print(\"global model loaded\")\n",
    "        \n",
    "\n",
    "        # call test\n",
    "        loss, accuracy = test(central_model, testloader, device)\n",
    "        return loss, {\"val_psnr\": accuracy}\n",
    "\n",
    "    return evaluate_fn\n",
    "\n",
    "\n",
    "# now we can define the strategy\n",
    "# strategy = fl.server.strategy.FedAvg(\n",
    "#     fraction_fit=0.1,\n",
    "#     fraction_evaluate=0.1,\n",
    "#     min_available_clients=100,\n",
    "#     evaluate_fn=get_evaluate_fn(testloader), # Even this is not required\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now define a strategy just as shown (commented) above. Instead, let's see how additional (but entirely optional) functionality can be easily added to our strategy. We are going to define two additional auxiliary functions to: (1) be able to configure how clients do local training; and (2) define a function to aggregate the metrics that clients return after running their `evaluate` methods:\n",
    "\n",
    "1. `fit_config()`. This is a function that will be executed inside the strategy when configuring a new `fit` round. This function is relatively simple and only requires as input argument the round at which the FL experiment is at. In this example we simply return a Python dictionary to specify the number of epochs and learning rate each client should made use of inside their `fit()` methods. A more versatile implementation would add more hyperparameters (e.g. the learning rate) and adjust them as the FL process advances (e.g. reducing the learning rate in later FL rounds).\n",
    "2. `weighted_average()`: This is an optional function to pass to the strategy. It will be executed after an evaluation round (i.e. when client run `evaluate()`) and will aggregate the metrics clients return. In this example, we use this function to compute the weighted average accuracy of clients doing `evaluate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr.common import Metrics\n",
    "\n",
    "\n",
    "def fit_config(server_round: int) -> Dict[str, Scalar]:\n",
    "    \"\"\"Return a configuration with static batch size and (local) epochs.\"\"\"\n",
    "    config = {\n",
    "        \"round\": server_round,\n",
    "        \"epochs\": 1,  # Number of local epochs done by clients\n",
    "        \"lr\": 0.01,  # Learning rate to use by clients during fit()\n",
    "    }\n",
    "    return config\n",
    "\n",
    "\n",
    "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
    "    \"\"\"Aggregation function for (federated) evaluation metrics, i.e. those returned by\n",
    "    the client's evaluate() method.\"\"\"\n",
    "    # Multiply accuracy of each client by number of examples used\n",
    "    accuracies = [num_examples * m[\"val_psnr\"] for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # Aggregate and return custom metric (weighted average)\n",
    "    return {\"val_psnr\": sum(accuracies) / sum(examples)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, Dict, List, Optional, Tuple\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    MetricsAggregationFn,\n",
    "    NDArrays,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
    "    def aggregate_fit(\n",
    "        self,\n",
    "        server_round: int,\n",
    "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
    "        failures: List[Union[Tuple[ClientProxy, FitRes], BaseException]],\n",
    "    ) -> Tuple[Optional[Parameters], Dict[str, Scalar]]:\n",
    "        \"\"\"Aggregate model weights using weighted average and store checkpoint\"\"\"\n",
    "\n",
    "        # Call aggregate_fit from base class (FedAvg) to aggregate parameters and metrics\n",
    "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
    "\n",
    "        if aggregated_parameters is not None:\n",
    "            print(f\"Saving round {server_round} aggregated_parameters...\")\n",
    "\n",
    "            # Convert `Parameters` to `List[np.ndarray]`\n",
    "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
    "\n",
    "            model = Net(hparams_central,full_dataset_central)\n",
    "            # Convert `List[np.ndarray]` to PyTorch`state_dict`\n",
    "            params_dict = zip(model.state_dict().keys(), aggregated_ndarrays)\n",
    "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
    "            model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), f\"ckpts/{hparams_central.exp_name}/server/round_{server_round}.pth\")\n",
    "\n",
    "        return aggregated_parameters, aggregated_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not hparams.public_dataset:\n",
    "    strategy = SaveModelStrategy(\n",
    "        fraction_fit=1.,  # Sample 10% of available clients for training\n",
    "        fraction_evaluate=0.1,  # Sample 5% of available clients for evaluation\n",
    "        min_fit_clients=4,  # Never sample less than 10 clients for training\n",
    "        min_evaluate_clients=1,  # Never sample less than 5 clients for evaluation\n",
    "        min_available_clients=int(\n",
    "            NUM_CLIENTS * 0.75\n",
    "        ),  # Wait until at least 75 clients are available\n",
    "        on_fit_config_fn=fit_config,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # aggregates federated metrics\n",
    "        evaluate_fn=get_evaluate_fn(valloaders[0]),  # global evaluation function\n",
    "    )\n",
    "else:\n",
    "    print(\"Initial Parameters given\")\n",
    "    strategy = SaveModelStrategy(\n",
    "        fraction_fit=1.,  # Sample 10% of available clients for training\n",
    "        fraction_evaluate=0.1,  # Sample 5% of available clients for evaluation\n",
    "        min_fit_clients=4,  # Never sample less than 10 clients for training\n",
    "        min_evaluate_clients=1,  # Never sample less than 5 clients for evaluation\n",
    "        min_available_clients=int(\n",
    "            NUM_CLIENTS * 0.75\n",
    "        ),  # Wait until at least 75 clients are available\n",
    "        on_fit_config_fn=fit_config,\n",
    "        evaluate_metrics_aggregation_fn=weighted_average,  # aggregates federated metrics\n",
    "        evaluate_fn=get_evaluate_fn(valloaders[0]),  # global evaluation function\n",
    "        initial_parameters=fl.common.ndarrays_to_parameters(public_model_static_params)#NOTE: finish this\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have:\n",
    "* created the dataset partitions (one for each client)\n",
    "* defined the client class\n",
    "* decided on a strategy to use\n",
    "\n",
    "Now we just need to launch the Flower FL experiment... not so fast! just one final function: let's create another callback that the Simulation Engine will use in order to span VirtualClients. As you can see this is really simple: construct a FlowerClient object, assigning each their own data partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_client_fn(trainloaders, valloaders,hparams,full_dataset):\n",
    "    def client_fn(cid: str):\n",
    "        \"\"\"Returns a FlowerClient containing the cid-th data partition\"\"\"\n",
    "\n",
    "        return FlowerClient(\n",
    "            hparams=hparams, trainloader=trainloaders[int(cid)], vallodaer=valloaders[int(cid)],full_dataset=full_dataset,client_number=int(cid)\n",
    "        )\n",
    "\n",
    "    return client_fn\n",
    "\n",
    "\n",
    "client_fn_callback = generate_client_fn(trainloaders, valloaders,hparams,full_dataset_central)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9ad8dcea-8004-4c6e-a025-e168da636c88"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-09-19 11:03:46,461 | app.py:175 | Starting Flower simulation, config: ServerConfig(num_rounds=30, round_timeout=None)\n",
      "2023-09-19 11:03:49,006\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2023-09-19 11:03:50,198 | app.py:210 | Flower VCE: Ray initialized with resources: {'node:__internal_head__': 1.0, 'memory': 861846494208.0, 'object_store_memory': 200000000000.0, 'CPU': 128.0, 'node:10.129.96.28': 1.0, 'accelerator_type:A100': 1.0, 'GPU': 2.0}\n",
      "INFO flwr 2023-09-19 11:03:50,199 | app.py:224 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 8, 'num_gpus': 0.5}\n",
      "INFO flwr 2023-09-19 11:03:50,217 | app.py:270 | Flower VCE: Creating VirtualClientEngineActorPool with 4 actors\n",
      "INFO flwr 2023-09-19 11:03:50,218 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2023-09-19 11:03:50,220 | server.py:276 | Requesting initial parameters from one random client\n",
      "INFO flwr 2023-09-19 11:03:55,450 | server.py:280 | Received initial parameters from one random client\n",
      "INFO flwr 2023-09-19 11:03:55,453 | server.py:91 | Evaluating initial parameters\n",
      "INFO flwr 2023-09-19 11:04:27,723 | server.py:94 | initial parameters (loss, other metrics): tensor(1.5754, device='cuda:0'), {'val_psnr': tensor(6.9991, device='cuda:0')}\n",
      "INFO flwr 2023-09-19 11:04:27,732 | server.py:104 | FL starting\n",
      "DEBUG flwr 2023-09-19 11:04:27,734 | server.py:222 | fit_round 1: strategy sampled 20 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 11:16:31,142 | server.py:236 | fit_round 1 received 20 results and 0 failures\n",
      "WARNING flwr 2023-09-19 11:16:31,320 | fedavg.py:242 | No fit_metrics_aggregation_fn provided\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 1 aggregated_parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-09-19 11:16:59,636 | server.py:125 | fit progress: (1, tensor(0.7406, device='cuda:0'), {'val_psnr': tensor(9.6018, device='cuda:0')}, 751.9026063049969)\n",
      "DEBUG flwr 2023-09-19 11:16:59,643 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 11:18:09,654 | server.py:187 | evaluate_round 1 received 2 results and 0 failures\n",
      "DEBUG flwr 2023-09-19 11:18:09,657 | server.py:222 | fit_round 2: strategy sampled 20 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 11:30:07,823 | server.py:236 | fit_round 2 received 20 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 2 aggregated_parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-09-19 11:30:36,899 | server.py:125 | fit progress: (2, tensor(0.4134, device='cuda:0'), {'val_psnr': tensor(13.7340, device='cuda:0')}, 1569.1650289289973)\n",
      "DEBUG flwr 2023-09-19 11:30:36,908 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 11:31:48,227 | server.py:187 | evaluate_round 2 received 2 results and 0 failures\n",
      "DEBUG flwr 2023-09-19 11:31:48,229 | server.py:222 | fit_round 3: strategy sampled 20 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 11:43:46,446 | server.py:236 | fit_round 3 received 20 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 3 aggregated_parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-09-19 11:44:15,750 | server.py:125 | fit progress: (3, tensor(0.1978, device='cuda:0'), {'val_psnr': tensor(16.3960, device='cuda:0')}, 2388.016537931995)\n",
      "DEBUG flwr 2023-09-19 11:44:15,757 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 11:45:25,972 | server.py:187 | evaluate_round 3 received 2 results and 0 failures\n",
      "DEBUG flwr 2023-09-19 11:45:25,977 | server.py:222 | fit_round 4: strategy sampled 20 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 11:57:33,827 | server.py:236 | fit_round 4 received 20 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 4 aggregated_parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-09-19 11:58:02,998 | server.py:125 | fit progress: (4, tensor(0.1377, device='cuda:0'), {'val_psnr': tensor(17.7429, device='cuda:0')}, 3215.264642119)\n",
      "DEBUG flwr 2023-09-19 11:58:03,008 | server.py:173 | evaluate_round 4: strategy sampled 2 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 11:59:14,262 | server.py:187 | evaluate_round 4 received 2 results and 0 failures\n",
      "DEBUG flwr 2023-09-19 11:59:14,265 | server.py:222 | fit_round 5: strategy sampled 20 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 12:11:14,452 | server.py:236 | fit_round 5 received 20 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 5 aggregated_parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-09-19 12:11:43,538 | server.py:125 | fit progress: (5, tensor(0.1058, device='cuda:0'), {'val_psnr': tensor(19.0254, device='cuda:0')}, 4035.8048292739986)\n",
      "DEBUG flwr 2023-09-19 12:11:43,547 | server.py:173 | evaluate_round 5: strategy sampled 2 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 12:12:54,287 | server.py:187 | evaluate_round 5 received 2 results and 0 failures\n",
      "DEBUG flwr 2023-09-19 12:12:54,290 | server.py:222 | fit_round 6: strategy sampled 20 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 12:24:52,502 | server.py:236 | fit_round 6 received 20 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 6 aggregated_parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-09-19 12:25:22,023 | server.py:125 | fit progress: (6, tensor(0.0852, device='cuda:0'), {'val_psnr': tensor(20.3616, device='cuda:0')}, 4854.28932542399)\n",
      "DEBUG flwr 2023-09-19 12:25:22,028 | server.py:173 | evaluate_round 6: strategy sampled 2 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 12:26:33,186 | server.py:187 | evaluate_round 6 received 2 results and 0 failures\n",
      "DEBUG flwr 2023-09-19 12:26:33,189 | server.py:222 | fit_round 7: strategy sampled 20 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 12:38:33,379 | server.py:236 | fit_round 7 received 20 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 7 aggregated_parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-09-19 12:39:02,970 | server.py:125 | fit progress: (7, tensor(0.0720, device='cuda:0'), {'val_psnr': tensor(21.4874, device='cuda:0')}, 5675.236932414002)\n",
      "DEBUG flwr 2023-09-19 12:39:02,976 | server.py:173 | evaluate_round 7: strategy sampled 2 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 12:40:14,021 | server.py:187 | evaluate_round 7 received 2 results and 0 failures\n",
      "DEBUG flwr 2023-09-19 12:40:14,023 | server.py:222 | fit_round 8: strategy sampled 20 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 12:52:11,725 | server.py:236 | fit_round 8 received 20 results and 0 failures\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving round 8 aggregated_parameters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2023-09-19 12:52:41,040 | server.py:125 | fit progress: (8, tensor(0.0634, device='cuda:0'), {'val_psnr': tensor(22.3006, device='cuda:0')}, 6493.306253212999)\n",
      "DEBUG flwr 2023-09-19 12:52:41,046 | server.py:173 | evaluate_round 8: strategy sampled 2 clients (out of 20)\n",
      "DEBUG flwr 2023-09-19 12:53:51,959 | server.py:187 | evaluate_round 8 received 2 results and 0 failures\n",
      "DEBUG flwr 2023-09-19 12:53:51,962 | server.py:222 | fit_round 9: strategy sampled 20 clients (out of 20)\n"
     ]
    }
   ],
   "source": [
    "logger_filename = f'{hparams.exp_name}.txt'\n",
    "fl.common.logger.configure(identifier=\"myFlowerExperiment\", filename=logger_filename)\n",
    "# With a dictionary, you tell Flower's VirtualClientEngine that each\n",
    "# client needs exclusive access to these many resources in order to run\n",
    "client_resources = {\"num_cpus\": 8, \"num_gpus\": 0.5}\n",
    "os.system(f'mkdir -p ckpts/{hparams.exp_name}/server')\n",
    "for i in range(NUM_CLIENTS):\n",
    "    os.system(f'mkdir -p ckpts/{hparams.exp_name}/clients/client_{i:0>2d}')\n",
    "    os.system(f'mkdir -p ckpts/{hparams.exp_name}/clients-static/client_{i:0>2d}')\n",
    "\n",
    "history = fl.simulation.start_simulation(\n",
    "    client_fn=client_fn_callback,  # a callback to construct a client\n",
    "    num_clients=NUM_CLIENTS,  # total number of clients in the experiment\n",
    "    config=fl.server.ServerConfig(num_rounds=hparams.num_rounds),  # let's run for 10 rounds\n",
    "    strategy=strategy,  # the strategy that will orchestrate the whole FL pipeline\n",
    "    client_resources=client_resources,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing 10 rounds should take less than 2 minutes on a CPU-only Colab instance <-- Flower Simulation is fast! ðŸš€\n",
    "\n",
    "You can then use the resturned `History` object to either save the results to disk or do some visualisation (or both of course, or neither if you like chaos). Below you can see how you can plot the centralised accuracy obtainined at the end of each round (including at the very beginning of the experiment) for the _global model_. This is want the function `evaluate_fn()` that we passed to the strategy reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "outputId": "d8eab106-cee9-4266-9082-0944882cdba8"
   },
   "outputs": [],
   "source": [
    "# print(f\"{history.metrics_centralized = }\")\n",
    "\n",
    "# global_accuracy_centralised = history.metrics_centralized[\"accuracy\"]\n",
    "# round = [data[0] for data in global_accuracy_centralised]\n",
    "# acc = [100.0 * data[1] for data in global_accuracy_centralised]\n",
    "# plt.plot(round, acc)\n",
    "# plt.grid()\n",
    "# plt.ylabel(\"Accuracy (%)\")\n",
    "# plt.xlabel(\"Round\")\n",
    "# plt.title(\"MNIST - IID - 100 clients with 10 clients per round\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
